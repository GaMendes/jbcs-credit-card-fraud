# Credit Card Fraud Detection with Explainability Comparison

This project implements and compares five machine learning models for credit card fraud detection. It evaluates model performance across three datasets and five random seeds. The feature absolute values of importance explanations generated by LIME and SHAP are compared against the coefficients of a Logistic Regression model (treated as ground truth). The project also analyzes the time taken for different stages, including explainer creation and explanation generation.

## Code Details

* **`main.py`**: Orchestrates the experiment runs across seeds, datasets, and models. Calls `ModelWrapper` and `ResultsSaver`.

* **`src/model_wrapper.py`**: Contains the `ModelWrapper` class based on the original structure, minimally modified to accept and use a `random_seed`. Handles the K-Fold loop, internal preprocessing calls, SMOTE, hyperparameter tuning, model fitting, *creation* of LIME/SHAP explainers, and metric calculation.

* **`src/utils.py`**: Includes helper functions for creating directories and the `ResultsSaver` class. `ResultsSaver` takes the output from `ModelWrapper`, *generates* the LIME/SHAP explanation values, extracts LogReg coefficients, and saves all intermediate CSV files (metrics, timings, explanations, importances).

* **`src/data_loader.py`**: Loads datasets based on names defined in `config.py`.

* **`src/config.py`**: Defines datasets, models, and hyperparameter grids used in the experiments.

* **`post_experiment_analysis.py`**: Standalone script containing the `MetricsAnalyzer` class. Reads the output CSVs generated by the experiments and calculates/saves final summary tables (metrics, agreement counts, agreement percentages, timings) in both CSV and LaTeX formats in the main `results/` directory.

## Project Structure

``` bash
.
├── data/                 # Placeholder for datasets (see data/README.md)
├── old/                  # Old files used in the experiments bofore the first review round
├── results/              # Stores all experiment outputs and summaries
│   ├── experiment<N>/    # Results for a specific random seed run (N = [1 ... 5])
│   │   └── <dataset_name>/
│   │       └── <model_name>/
│   │           ├── metrics.csv                             # Performance metrics per fold
│   │           ├── timings.csv                             # Detailed timings per fold (incl. explanation generation)
│   │           ├── model_importances.csv                   # Only for Logistic Regression, stores the coefficients per fold
│   │           ├── lime_explanations.csv                   # LIME feature importance values per fold/instance
│   │           ├── shap_explanations.csv                   # SHAP feature importance values per fold/instance
│   │           └── mean_metrics_summary_per_run.csv        # Mean metrics for this specific run
│   │       └── all_algorithms_dataset_mean_metrics.csv     # Mean metrics for all algos on this dataset/run
│   │       └── agreement_counts.csv                        # Intersection counts (LIME/SHAP vs LogReg) per fold
│   │       └── top_features_for_validation.csv             # Top 10 features (LIME/SHAP) ordered by importance
│   ├── plots                                               # Plots for all algorithms for the first seed by dataset
│   ├── overall_metrics_summary_with_std.csv                # Mean metrics across all runs/seeds with standard deviation
│   ├── overall_agreement_total_counts_table.csv            # Sum of agreement counts across all runs/seeds
│   ├── overall_agreement_total_percentage_table.csv        # Percentage of agreement counts across all runs/seeds
│   ├── overall_agreement_total_percentage_table.csv        # Agreement percentage across all runs/seeds
│   ├── <dataset_name>_timing_summary.csv                   # Mean timing summary per dataset
│
│
│── config.py                       # Model, parameter, and dataset configurations
│── data_loader.py                  # Loads specified datasets
│── log_ieee_cis.txt                # Logs for the execution of the experiment pipeline for the ieee-cis dataset (We had to run experiments for it a second time, because of an id field missed in the data on the first execution)
│── logs.txt                        # Logs for the execution of the experiment pipeline
│── data_loader.py                  # Loads specified datasets
│── model_wrapper.py                # Core ModelWrapper class that implements most functions related to the experiments
│── plots.py                        # Implements functions to plot data
│── utils.py                        # Utility functions (directory creation, ResultsSaver class)
├── main.py                         # Main script to run experiments
└── post_experiment_analysis.py     # Script to analyze results and generate summaries/LaTeX tables

```
## Setup

1.  **Clone the repository:**

    ```bash
    git clone <repository-url>
    cd <repository-name>
    ```

2.  **Create Python Environment (Recommended):**

    ```bash
    python -m venv venv
    source venv/bin/activate
    ```

3.  **Install Dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

4.  **Obtain Datasets:**
    * Due to licensing restrictions, the datasets are not included in this repository.
    * Please see the instructions in `data/README.md` to download the required datasets and place them in the `data/` folder. The expected filenames are:
        * `creditcard.csv` (for 'kaggle' dataset)
        * `train_identity.csv` and `train_transaction.csv` (for 'ieee-cis' dataset)
        * `transactions_480m.csv` (for the 'synthetic' dataset - adjust filename in `src/data_loader.py` if different)

## Running the Project

The project involves two main steps: running the experiments and then analyzing the results.

**Step 1: Run Experiments**

1.  **Configure Experiments (Optional):**

    * Modify `src/config.py` to change datasets, models, or hyperparameter grids.

2.  **Run the Main Script:**

    * Ensure you are in the project's root directory.
    * Execute the command:

        ```bash
        python main.py
        ```
    * This will populate the `results/experiment<N>/` directories with detailed outputs for each run.

**Step 2: Analyze Results and Generate Summaries**

1.  **Run the Post-Analysis Script:**

    * Ensure you are in the project's root directory.
    * Execute the command:

        ```bash
        python post_experiment_analysis.py
        ```
    * This script will:
        * Calculate agreement counts and top features per experiment/dataset (saved within `results/experiment<N>/<dataset_name>/`).
        * Calculate and save overall agreement tables (counts and percentages) to the main `results/` folder.
        * Calculate and save timing summary tables per dataset to the main `results/` folder.
        * Calculate and save the overall metrics summary to the main `results/` folder.
        * Fix issues with LIME feature names